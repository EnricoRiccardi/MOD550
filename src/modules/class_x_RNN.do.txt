
!split
======= Recursive Neural Network =======


!split
===== Sequential Data =====

!bblock ORDER MATTERS
* Language Models
* Time series
!eblock

\pause

!bblock Language model
* Prediction of the next word
* Prediction of next sentence
!eblock

\pause

!bblock Time Series
* Weather data
* Stock market
* Monitoring
* Trajectories
* Etc
!eblock

!split
===== Feedforward (FF) vs Recurrent NN (RNN) =====
!bblock FF network
* One set of input
* One set of output
* Different parameters at each layer
!eblock

!bblock
* Multiple input set
* Multiple output
* Same parameter set
!eblock

!split
===== NN =====
FIGURE: [../figures/1step_NN, fract=1]


!split
===== NN =====
FIGURE: [../figures/NN_more_nodes, fract=1]


!split
===== NN =====
FIGURE: [../figures/NN_act, fract=1]

!split
===== NN =====
FIGURE: [../figures/Activation_functions, fract=1]

!split
===== NN =====
FIGURE: [../figures/NN_growing, fract=1]


!split
===== NN =====
FIGURE: [../figures/NN_image_recogn, fract=1]


!split
===== RNN =====
FIGURE: [../figures/RNN, fract=1]



!split
===== RNN =====
!bblock Advantages
* Model size is fixed
* Each info is stored/learned
* The weights can be forwarded
!eblock

/pause

!bblock Problems
* Computationally demanding: long training times
* Problematic with Long series
* It can diverge (explode) or gradient vanish
* It cannot be very deep
* Unable to handle long time dependencies
!eblock

!split
===== RNN problems =====

FIGURE: [../figures/RCC_problems, fract=1.2]

!split
===== RNN problems =====
!bblock Exploding gradients
* Large weights update
* Gradient descent diverge (solution method)
!eblock

\pause

!bblock Vanishing gradients
* Weights get marginally upgraded
* Very slow convergence speed
!eblock


!split
===== LSTM: Long Short Term Memory =====
!bblock NB...
Filters forget data...
!eblock

What about if we purposely forget data?

/pause
LSTM includes Forget Gates


!bblock Automatic filter!
The forget gates learn to forget what is not interesting
!eblock

This is extremely useful but also rather worrisome: you have no control!

/pause

LSTM is an advanced version of GRU (Gated Recurrent Units)...
What is GRU?


!split
===== GRU =====
FIGURE: [../figures/GRU_comic, fract=1]

!split
===== GRU =====
FIGURE: [../figures/GRU_1, fract=1]


!split
===== GRU =====
!bblock Reset gates
To capture short-term dependencies
!eblock

/pause

!bblock Update gates
To capture Long-term dependencies
!eblock

pause
Each gate has its own weight

!split
===== RNN =====
FIGURE: [../figures/RNN_Core, fract=0.9]

$x_t$: input vector, $h_t$: hidden layer vector
$o_t$: output vector

!split
===== GRU =====
FIGURE: [../figures/GRU, fract=0.8]

$x_t$: input vector, $h_t$: hidden layer vector
$o_t$: output vector, $r_t$: reset factors, $z_t$: update factors


!split
===== LSTM =====
FIGURE: [../figures/LSTM, fract=1]

$x_t$: input vector, $h_t, C_t$: hidden layer vector
$o_t$: output vector, $r_t$: reset factors, $z_t$: update factors



