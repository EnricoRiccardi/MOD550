!split
======= Model evaluation =======

We can measure the accuracy of a prediction only when we have a 'truth' to
compare with.

!bblock Model outcome
The accuracy of a prediction is limited by the available data set (not an universal quantity).
!eblock


Given a dataset, we split the data in a \alert{train} and a {test}Â set.

One has to be extremely careful to not introduce a _bias_ in each of them when splitting.
(or to introduce a bias properly, when for example, working with time seriers. 



!split
===== Train =====

A train dataset is a subset of the original data. Generally it is circa a 70 \% (common practice, not a rule) of the 
*INSTANCES*.

For each selected instance, all the features are kept, as their labels.

!bblock Random measurements
Some models are able to consider also the sequence of instances, the selection of the \alert{train} dataset has to be randomized.
!eblock

The opposite consideration shall be done for time series, where each data point is correlated to the previous.


!split
===== Validation =====
A validation dataset is a subset of the original data. Generally it is circa a 15 \% (common practice, not a rule) of the *INSTANCES*.

It is aimed to provide a /alert{unbiased estimate of the model prediction error}. This information is then used
to *tune* the model set up (indirectly). I.e. it is not independend from the test phase.

For each selected instance, all the features are kept, as their labels.

!bblock Random measurements
Some models are able to consider also the sequence of instances, the selection of the \alert{validation} dataset has to be randomized.
!eblock

The opposite consideration shall be done for time series, where each data point is correlated to the previous.


!split
===== Test =====

A test dataset is a subset of the original data. Generally it is circa a 70 \% (common practice, not a rule) of the 
*INSTANCES*.

For each selected instance, all the features are kept, as their labels.

!bblock Random measurements
Some models are able to consider also the sequence of instances, the election of the alert{test} dataset has to be randomized.
!eblock



!split
===== Model evaluation =====

Train-Validation-Test
!bpop
o Data is split into train, validation and test dataset
o A model is trained on the train dataset
o The model (hyper)parameters are improved on the validation dataset
o The model performances (e.g. accuracy) are tested on the test dataset
!epop

FIGURE: [../figures/test-validation-train, frac=0.9]

!split
===== Cross validation =====

Cross validation-Test
!bpop
o Data is split into train and test. 
o Train dataset is then split into k-folds (different subsets, usually 5 to 10)
o A model is trained on all k subset but one, sequentially
o The model is tested on the remaining  k data subset and its output averaged
o The model performances (e.g. accuracy) are tested on the test dataset
!epop

FIGURE: [../figures/cross_validation, frac=1]



!split
===== Model performance metrics =====

One of the main core concepts behind machine learning is the _loss function_.

In mathematical optimization and decision theory, a loss function or cost function (sometimes also called an error function) is a function that maps an event or values of one or more variables onto a real number intuitively representing some "cost" associated with the event. [Wiki]


\pause


!bblock Loss function in Machine Learning
It quantifies the difference between the predicted outputs of a machine learning algorithm and the actual target values.
!eblock




!split
===== Loss function aims =====


It provides a set of core quantifications/possibilities:


!bpop
o Performance measurement: it provides the metric of the prediction performances


o Direction for improvement: it allows the identification of convergent solutions


o Balancing bias and variance: it allows to account for artefact in the sampling phase


o Influencing model behaviour: it allows to bridge data driven methods with mathematics/physics
!epop




!split
===== Most common loss function in machine learning =====


Most popular entries:
!bpop
o Mean Square Error (regression): Fast computations, good convergence.


o Mean Absolute Error (regression): No focus on the outliers, poor convergence.


o Entropy Loss (classification): measures the difference between the model probability distribution outcomes and the predicted values


!epop


!split
===== Losses =====


!bblock Mean absolute error, L1 loss
!bt
\[
MAE = \frac{1}{n} \sum_{i=1}^n (|y_i -\bar y|)
\]
!et
!eblock


\space


!bblock Mean square error, L2 loss
!bt
\[
MSE = \frac{1}{n} \sum_{i=1}^n (y_i -\bar y)^2
\]
!et
!eblock


!bblock Log Loss for binary classification
!bt
\[
Log\_Loss = -[y \ log(f(x)) + (1-y) log(1-f(x))]
\]
!et
!eblock




!split
===== MSE in Vanilla Python =====


!bblock
@@@CODE ../code/mse_vanilla.py fromto: def mean@# Loop
!eblock




!split
===== MSE in Vanilla Python =====


!bblock
@@@CODE ../code/mse_vanilla.py fromto:sum_squared_error@
!eblock




!split
===== MSE in Numpy Python =====
!bblock
@@@CODE ../code/mse_numpy.py
!eblock


!split
===== MSE in Python =====
!bblock
@@@CODE ../code/mse.py
!eblock


!split
===== MSE in benchmark =====
!bblock
@@@CODE ../code/mse_scaling.py
!eblock


!split
===== Coding MSE =====
!bblock
@@@CODE ../code/mse_scaling_2.py
!eblock


!split
===== Coding MSE =====
!bblock
@@@CODE ../code/mse_scaling_3.py
!eblock



